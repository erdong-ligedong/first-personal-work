# first-personal-work  
1.数据采集（comment.py):通过爬虫获取《在一起》的全部评论信息  
具体思路：  
获取url地址-->观察url之间的变化规律-->遍历url-->正则提取关键字-->保存结果为comments.json格式文件  
遇到的问题：  
    a.url之间的规律很隐蔽，需要很认真才发现规律  
    b.对爬虫的知识掌握的并不牢固，还需要继续学习爬虫的相关知识  
    c.了解了url之间变化的规律，但是不知道该怎么全部爬取下来  
  
2.数据分析（high.py）:提取高频词汇，保存为result.json文件  
具体思路：  
运用jieba分词提取高频词汇  
遇到的问题：  
jieba是第一次接触的新知识，对它的使用功能还并不熟悉，需要进行更深的学习 
  
3.生成词云（cloud.html）：运用echarts.js,echarts-worldcloud.min.js插件生成想要的词云模样  
具体思路：  
利用pycharm生成一个.html，向其中添加生成词云的代码  
遇到的问题：  
遇到最大的问题就是这些东西都是新接触的，短时间内没办法完整的吸收
